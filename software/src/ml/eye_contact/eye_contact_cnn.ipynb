{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eye-contact-cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rehg-lab/eye-contact-cnn.git"
      ],
      "metadata": {
        "id": "dk96YUHq9FUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colour Pillow dlib numpy torch torchvision pytz typing-extensions six"
      ],
      "metadata": {
        "id": "dDSGsqU2xctF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "print(torch.cuda.device_count())\n",
        "assert(torch.cuda.device_count() > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8weLYcfy0at",
        "outputId": "8f71f924-bf5b-4c0d-b6ae-0f0f9ae80a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.py - Modified"
      ],
      "metadata": {
        "id": "g90xiLMOyqVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def model_static(pretrained=False, **kwargs):\n",
        "    model = ResNet([3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        print('loading saved model weights')\n",
        "        model_dict = model.state_dict()\n",
        "        snapshot = torch.load(pretrained)\n",
        "        snapshot = {k: v for k, v in snapshot.items() if k in model_dict}\n",
        "        model_dict.update(snapshot)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3,\n",
        "                               bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer1 = self._make_layer(64, layers[0])\n",
        "        self.layer2 = self._make_layer(128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride = 1)\n",
        "        self.fc_theta = nn.Linear(512 * Bottleneck.expansion, 34)\n",
        "        self.fc_phi = nn.Linear(512 * Bottleneck.expansion, 34)\n",
        "        self.fc_ec = nn.Linear(512 * Bottleneck.expansion, 1)\n",
        "        self.init_param()\n",
        "\n",
        "    def init_param(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.shape[0] * m.weight.shape[1]\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride = 1):\n",
        "        downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * Bottleneck.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * Bottleneck.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * Bottleneck.expansion),\n",
        "                )\n",
        "\n",
        "        layers.append(Bottleneck(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * Bottleneck.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(Bottleneck(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_ec(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "FOX6wZX3yAdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "demo.py"
      ],
      "metadata": {
        "id": "Rt7DVRJQygsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import argparse, os, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from model import model_static\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from colour import Color\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def bbox_jitter(bbox_left, bbox_top, bbox_right, bbox_bottom):\n",
        "    cx = (bbox_right+bbox_left)/2.0\n",
        "    cy = (bbox_bottom+bbox_top)/2.0\n",
        "    scale = random.uniform(0.8, 1.2)\n",
        "    bbox_right = (bbox_right-cx)*scale + cx\n",
        "    bbox_left = (bbox_left-cx)*scale + cx\n",
        "    bbox_top = (bbox_top-cy)*scale + cy\n",
        "    bbox_bottom = (bbox_bottom-cy)*scale + cy\n",
        "    return bbox_left, bbox_top, bbox_right, bbox_bottom\n",
        "\n",
        "\n",
        "def drawrect(drawcontext, xy, outline=None, width=0):\n",
        "    (x1, y1), (x2, y2) = xy\n",
        "    points = (x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)\n",
        "    drawcontext.line(points, fill=outline, width=width)\n",
        "\n",
        "\n",
        "def run(video_path, face_path, model_weight, jitter, vis, display_off, save_text):\n",
        "    # parser = argparse.ArgumentParser()\n",
        "\n",
        "    # parser.add_argument('--video', type=str, help='input video path. live cam is used when not specified')\n",
        "    # parser.add_argument('--face', type=str, help='face detection file path. dlib face detector is used when not specified')\n",
        "    # parser.add_argument('--model_weight', type=str, help='path to model weights file', default='data/model_weights.pkl')\n",
        "    # parser.add_argument('--jitter', type=int, help='jitter bbox n times, and average results', default=0)\n",
        "    # parser.add_argument('-save_vis', help='saves output as video', action='store_true')\n",
        "    # parser.add_argument('-save_text', help='saves output as text', action='store_true')\n",
        "    # parser.add_argument('-display_off', help='do not display frames', action='store_true')\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    CNN_FACE_MODEL = '/content/eye-contact-cnn/data/mmod_human_face_detector.dat' # from http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
        "\n",
        "    # set up vis settings\n",
        "    red = Color(\"red\")\n",
        "    colors = list(red.range_to(Color(\"green\"),10))\n",
        "    font = ImageFont.truetype(\"/content/eye-contact-cnn/data/arial.ttf\", 40)\n",
        "\n",
        "    # set up video source\n",
        "    if video_path is None:\n",
        "        # Look into this... might be -1?\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        video_path = 'live.avi'\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # set up output file\n",
        "    if save_text:\n",
        "        outtext_name = os.path.basename(video_path).replace('.avi','_output.txt')\n",
        "        f = open(outtext_name, \"w\")\n",
        "        print(f\"Output text file path: {outtext_name}\")\n",
        "    if vis:\n",
        "        outvis_name = os.path.basename(video_path).replace('.avi','_output.avi')\n",
        "        print(f\"Output video file path: {outvis_name}\")\n",
        "        imwidth = int(cap.get(3))\n",
        "        imheight = int(cap.get(4))\n",
        "        outvid = cv2.VideoWriter(outvis_name,cv2.VideoWriter_fourcc('M','J','P','G'), cap.get(5), (imwidth,imheight))\n",
        "\n",
        "    # set up face detection mode\n",
        "    if face_path is None:\n",
        "        facemode = 'DLIB'\n",
        "    else:\n",
        "        facemode = 'GIVEN'\n",
        "        column_names = ['frame', 'left', 'top', 'right', 'bottom']\n",
        "        df = pd.read_csv(face_path, names=column_names, index_col=0)\n",
        "        df['left'] -= (df['right']-df['left'])*0.2\n",
        "        df['right'] += (df['right']-df['left'])*0.2\n",
        "        df['top'] -= (df['bottom']-df['top'])*0.1\n",
        "        df['bottom'] += (df['bottom']-df['top'])*0.1\n",
        "        df['left'] = df['left'].astype('int')\n",
        "        df['top'] = df['top'].astype('int')\n",
        "        df['right'] = df['right'].astype('int')\n",
        "        df['bottom'] = df['bottom'].astype('int')\n",
        "\n",
        "    if (cap.isOpened()== False):\n",
        "        print(\"Error opening video stream or file\")\n",
        "        exit()\n",
        "\n",
        "    if facemode == 'DLIB':\n",
        "        cnn_face_detector = dlib.cnn_face_detection_model_v1(CNN_FACE_MODEL)\n",
        "    frame_cnt = 0\n",
        "\n",
        "    # set up data transformation\n",
        "    test_transforms = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    # load model weights\n",
        "    model = model_static(model_weight)\n",
        "    model_dict = model.state_dict()\n",
        "    snapshot = torch.load(model_weight)\n",
        "    model_dict.update(snapshot)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    model.cuda()\n",
        "    model.train(False)\n",
        "\n",
        "    # video reading loop\n",
        "    while(cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "            height, width, channels = frame.shape\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            frame_cnt += 1\n",
        "            bbox = []\n",
        "            if facemode == 'DLIB':\n",
        "                dets = cnn_face_detector(frame, 1)\n",
        "                for d in dets:\n",
        "                    l = d.rect.left()\n",
        "                    r = d.rect.right()\n",
        "                    t = d.rect.top()\n",
        "                    b = d.rect.bottom()\n",
        "                    # expand a bit\n",
        "                    l -= (r-l)*0.2\n",
        "                    r += (r-l)*0.2\n",
        "                    t -= (b-t)*0.2\n",
        "                    b += (b-t)*0.2\n",
        "                    bbox.append([l,t,r,b])\n",
        "            elif facemode == 'GIVEN':\n",
        "                if frame_cnt in df.index:\n",
        "                    bbox.append([df.loc[frame_cnt,'left'],df.loc[frame_cnt,'top'],df.loc[frame_cnt,'right'],df.loc[frame_cnt,'bottom']])\n",
        "\n",
        "            frame = Image.fromarray(frame)\n",
        "            for b in bbox:\n",
        "                face = frame.crop((b))\n",
        "                img = test_transforms(face)\n",
        "                img.unsqueeze_(0)\n",
        "                if jitter > 0:\n",
        "                    for i in range(jitter):\n",
        "                        bj_left, bj_top, bj_right, bj_bottom = bbox_jitter(b[0], b[1], b[2], b[3])\n",
        "                        bj = [bj_left, bj_top, bj_right, bj_bottom]\n",
        "                        facej = frame.crop((bj))\n",
        "                        img_jittered = test_transforms(facej)\n",
        "                        img_jittered.unsqueeze_(0)\n",
        "                        img = torch.cat([img, img_jittered])\n",
        "\n",
        "                # forward pass\n",
        "                output = model(img.cuda())\n",
        "                if jitter > 0:\n",
        "                    output = torch.mean(output, 0)\n",
        "                score = torch.sigmoid(output).item()\n",
        "\n",
        "                coloridx = min(int(round(score*10)),9)\n",
        "                draw = ImageDraw.Draw(frame)\n",
        "                drawrect(draw, [(b[0], b[1]), (b[2], b[3])], outline=colors[coloridx].hex, width=5)\n",
        "                draw.text((b[0],b[3]), str(round(score,2)), fill=(255,255,255,128), font=font)\n",
        "                if save_text:\n",
        "                    f.write(\"%d,%f\\n\"%(frame_cnt,score))\n",
        "\n",
        "            if not display_off:\n",
        "                frame = np.asarray(frame) # convert PIL image back to opencv format for faster display\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                cv2_imshow(frame)\n",
        "                # cv2.imshow('',frame)\n",
        "                if vis:\n",
        "                    outvid.write(frame)\n",
        "                key = cv2.waitKey(1) & 0xFF\n",
        "                if key == ord('q'):\n",
        "                    break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if vis:\n",
        "        outvid.release()\n",
        "    if save_text:\n",
        "        f.close()\n",
        "    cap.release()\n",
        "    print('DONE!')"
      ],
      "metadata": {
        "id": "gXGJtE59ygDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_person_talking():\n",
        "  # video_path = input video path. live cam is used when not specified'\n",
        "  # face_path = face detection file path. dlib face detector is used when not specified\n",
        "  # model_weight = path to model weights file, default: 'data/model_weights.pkl'\n",
        "  # jitter = jitter bbox n times and average results, default: 0\n",
        "  # save_vis = saves output as video\n",
        "  # save_text = saves output as text\n",
        "  # display_off = do not display frames\n",
        "\n",
        "  run(video_path='/content/person_talking.avi', face_path=None, model_weight='/content/eye-contact-cnn/data/model_weights.pkl', jitter=0, vis=True, save_text=True, display_off=False)"
      ],
      "metadata": {
        "id": "xv3h8O2wBdkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_person_talking()"
      ],
      "metadata": {
        "id": "TsPBa08Fyw8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}