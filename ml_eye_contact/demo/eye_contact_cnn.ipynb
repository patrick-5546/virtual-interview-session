{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91nmcs1q5XCr"
      },
      "source": [
        "# Dependency Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk96YUHq9FUW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/rehg-lab/eye-contact-cnn.git\n",
        "!git clone https://github.com/chrischang5/eye-contact-demo-files.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDSGsqU2xctF"
      },
      "outputs": [],
      "source": [
        "!pip install colour Pillow dlib numpy torch torchvision pytz typing-extensions six"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqGadlJ_goR"
      },
      "source": [
        "Make sure hardware acceleration is turned on by going to Edit > Notebook settings > Selecting 'GPU' from the hardware accelerator dropdown\n",
        "\n",
        "This assertion verifies that.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W8weLYcfy0at"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "assert(torch.cuda.device_count() > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90xiLMOyqVo"
      },
      "source": [
        "# Model and Demo Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FOX6wZX3yAdF"
      },
      "outputs": [],
      "source": [
        "# Code adapted from https://github.com/rehg-lab/eye-contact-cnn\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import dlib\n",
        "import cv2\n",
        "import argparse, os, random\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from colour import Color\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def model_static(pretrained=False, **kwargs):\n",
        "    model = ResNet([3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        print('Loading saved model weights...')\n",
        "        model_dict = model.state_dict()\n",
        "        snapshot = torch.load(pretrained)\n",
        "        snapshot = {k: v for k, v in snapshot.items() if k in model_dict}\n",
        "        model_dict.update(snapshot)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3,\n",
        "                               bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer1 = self._make_layer(64, layers[0])\n",
        "        self.layer2 = self._make_layer(128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride = 1)\n",
        "        self.fc_theta = nn.Linear(512 * Bottleneck.expansion, 34)\n",
        "        self.fc_phi = nn.Linear(512 * Bottleneck.expansion, 34)\n",
        "        self.fc_ec = nn.Linear(512 * Bottleneck.expansion, 1)\n",
        "        self.init_param()\n",
        "\n",
        "    def init_param(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.shape[0] * m.weight.shape[1]\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride = 1):\n",
        "        downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * Bottleneck.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * Bottleneck.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * Bottleneck.expansion),\n",
        "                )\n",
        "\n",
        "        layers.append(Bottleneck(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * Bottleneck.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(Bottleneck(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_ec(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gXGJtE59ygDb"
      },
      "outputs": [],
      "source": [
        "def bbox_jitter(bbox_left, bbox_top, bbox_right, bbox_bottom):\n",
        "    cx = (bbox_right+bbox_left)/2.0\n",
        "    cy = (bbox_bottom+bbox_top)/2.0\n",
        "    scale = random.uniform(0.8, 1.2)\n",
        "    bbox_right = (bbox_right-cx)*scale + cx\n",
        "    bbox_left = (bbox_left-cx)*scale + cx\n",
        "    bbox_top = (bbox_top-cy)*scale + cy\n",
        "    bbox_bottom = (bbox_bottom-cy)*scale + cy\n",
        "    return bbox_left, bbox_top, bbox_right, bbox_bottom\n",
        "\n",
        "def drawrect(drawcontext, xy, outline=None, width=0):\n",
        "    (x1, y1), (x2, y2) = xy\n",
        "    points = (x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)\n",
        "    drawcontext.line(points, fill=outline, width=width)\n",
        "\n",
        "def run(video_path, face_path, jitter, display_off, save_text):\n",
        "    '''\n",
        "      params:\n",
        "      - video_path = input video path. live cam is used when not specified'\n",
        "      - face_path = face detection file path. dlib face detector is used when not specified\n",
        "      - jitter = jitter bbox n times and average results, default: 0\n",
        "      - save_text = saves output as text\n",
        "      - display_off = do not display frames\n",
        "    '''\n",
        "    \n",
        "    CNN_FACE_MODEL = '/content/eye-contact-cnn/data/mmod_human_face_detector.dat' # from http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
        "\n",
        "    # set up vis settings\n",
        "    red = Color(\"red\")\n",
        "    colors = list(red.range_to(Color(\"green\"),10))\n",
        "    font = ImageFont.truetype(\"/content/eye-contact-cnn/data/arial.ttf\", 40)\n",
        "    model_weight = '/content/eye-contact-cnn/data/model_weights.pkl'\n",
        "\n",
        "    # set up video source\n",
        "    if video_path is None:\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        video_path = 'live.avi'\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # set up output file\n",
        "    if save_text:\n",
        "        outtext_name = os.path.basename(video_path).replace('.','_') + \".txt\"\n",
        "        f = open(outtext_name, \"w\")\n",
        "        print(f\"Output text file path: {outtext_name}\")\n",
        "\n",
        "    # set up face detection mode\n",
        "    if face_path is None:\n",
        "        facemode = 'DLIB'\n",
        "    else:\n",
        "        facemode = 'GIVEN'\n",
        "        column_names = ['frame', 'left', 'top', 'right', 'bottom']\n",
        "        df = pd.read_csv(face_path, names=column_names, index_col=0)\n",
        "        df['left'] -= (df['right']-df['left'])*0.2\n",
        "        df['right'] += (df['right']-df['left'])*0.2\n",
        "        df['top'] -= (df['bottom']-df['top'])*0.1\n",
        "        df['bottom'] += (df['bottom']-df['top'])*0.1\n",
        "        df['left'] = df['left'].astype('int')\n",
        "        df['top'] = df['top'].astype('int')\n",
        "        df['right'] = df['right'].astype('int')\n",
        "        df['bottom'] = df['bottom'].astype('int')\n",
        "\n",
        "    if (cap.isOpened()== False):\n",
        "        print(\"Error opening video stream or file\")\n",
        "        exit()\n",
        "\n",
        "    if facemode == 'DLIB':\n",
        "        cnn_face_detector = dlib.cnn_face_detection_model_v1(CNN_FACE_MODEL)\n",
        "    frame_cnt = 0\n",
        "\n",
        "    # set up data transformation\n",
        "    test_transforms = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    # load model weights\n",
        "    model = model_static(model_weight)\n",
        "    model_dict = model.state_dict()\n",
        "    snapshot = torch.load(model_weight)\n",
        "    model_dict.update(snapshot)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    model.cuda()\n",
        "    model.train(False)\n",
        "\n",
        "    # video reading loop\n",
        "    while(cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "            height, width, channels = frame.shape\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            frame_cnt += 1\n",
        "            bbox = []\n",
        "            if facemode == 'DLIB':\n",
        "                dets = cnn_face_detector(frame, 1)\n",
        "                for d in dets:\n",
        "                    l = d.rect.left()\n",
        "                    r = d.rect.right()\n",
        "                    t = d.rect.top()\n",
        "                    b = d.rect.bottom()\n",
        "                    # expand a bit\n",
        "                    l -= (r-l)*0.2\n",
        "                    r += (r-l)*0.2\n",
        "                    t -= (b-t)*0.2\n",
        "                    b += (b-t)*0.2\n",
        "                    bbox.append([l,t,r,b])\n",
        "            elif facemode == 'GIVEN':\n",
        "                if frame_cnt in df.index:\n",
        "                    bbox.append([df.loc[frame_cnt,'left'],df.loc[frame_cnt,'top'],df.loc[frame_cnt,'right'],df.loc[frame_cnt,'bottom']])\n",
        "\n",
        "            frame = Image.fromarray(frame)\n",
        "            for b in bbox:\n",
        "                face = frame.crop((b))\n",
        "                img = test_transforms(face)\n",
        "                img.unsqueeze_(0)\n",
        "                if jitter > 0:\n",
        "                    for i in range(jitter):\n",
        "                        bj_left, bj_top, bj_right, bj_bottom = bbox_jitter(b[0], b[1], b[2], b[3])\n",
        "                        bj = [bj_left, bj_top, bj_right, bj_bottom]\n",
        "                        facej = frame.crop((bj))\n",
        "                        img_jittered = test_transforms(facej)\n",
        "                        img_jittered.unsqueeze_(0)\n",
        "                        img = torch.cat([img, img_jittered])\n",
        "\n",
        "                # forward pass\n",
        "                output = model(img.cuda())\n",
        "                if jitter > 0:\n",
        "                    output = torch.mean(output, 0)\n",
        "                score = torch.sigmoid(output).item()\n",
        "\n",
        "                coloridx = min(int(round(score*10)),9)\n",
        "                draw = ImageDraw.Draw(frame)\n",
        "                drawrect(draw, [(b[0], b[1]), (b[2], b[3])], outline=colors[coloridx].hex, width=5)\n",
        "                draw.text((b[0],b[3]), str(round(score,2)), fill=(255,255,255,128), font=font)\n",
        "                if save_text:\n",
        "                    f.write(\"%d,%f\\n\"%(frame_cnt,score))\n",
        "\n",
        "            if not display_off:\n",
        "                frame = np.asarray(frame) # convert PIL image back to opencv format for faster display\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                cv2_imshow(frame)\n",
        "                key = cv2.waitKey(1) & 0xFF\n",
        "                if key == ord('q'):\n",
        "                    break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if save_text:\n",
        "        f.close()\n",
        "    cap.release()\n",
        "    print('DONE!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xv3h8O2wBdkH"
      },
      "outputs": [],
      "source": [
        "def run_person_talking(frame_display=True):\n",
        "  run(video_path='/content/eye-contact-demo-files/person_talking.avi', face_path=None, jitter=0, save_text=True, display_off=not frame_display)\n",
        "\n",
        "def run_single_images(frame_display=True):\n",
        "  for i in range(0,5):\n",
        "    run(video_path=f'/content/eye-contact-demo-files/{i}.jpg', face_path=None, jitter=0, save_text=True, display_off= not frame_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WRSbPnr_YYU"
      },
      "source": [
        "# Saving Model Weights to .txt File\n",
        "\n",
        "This method saves the eye contact model weights to a text file. No need to run this as the model has been saved already and it takes a while to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rZo_a6e6_BLs"
      },
      "outputs": [],
      "source": [
        "def write_model_weights():\n",
        "  '''\n",
        "    Writes model weights to a text file\n",
        "    This has been done before! So do not run this method as this takes a long time.\n",
        "  '''\n",
        "  model_weight = '/content/eye-contact-cnn/data/model_weights.pkl'\n",
        "\n",
        "  model = model_static(model_weight)\n",
        "  model_dict = model.state_dict()\n",
        "  snapshot = torch.load(model_weight)\n",
        "  model_dict.update(snapshot)\n",
        "  model.load_state_dict(model_dict)\n",
        "\n",
        "  f = open(\"weights.txt\", \"w\")\n",
        "  torch.set_printoptions(profile=\"full\")\n",
        "\n",
        "  for param_tensor in model.state_dict():\n",
        "    f.write(f\"{param_tensor} \\t {model.state_dict()[param_tensor].size()}, {model.state_dict()[param_tensor]}\\n\")\n",
        "\n",
        "  torch.set_printoptions(profile=\"default\")\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZz5ZSxd_T-m"
      },
      "source": [
        "# Demos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsPBa08Fyw8o"
      },
      "outputs": [],
      "source": [
        "run_person_talking(frame_display=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "563oOY4LsnuU"
      },
      "outputs": [],
      "source": [
        "run_single_images(frame_display=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_WRSbPnr_YYU"
      ],
      "name": "eye-contact-cnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}